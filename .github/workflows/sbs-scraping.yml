name: Scraping SBS Diario

on:
  schedule:
    - cron: '0 5 * * *'  # 12am Hora Perú
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout del código
      uses: actions/checkout@v4

    - name: Configurar Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Instalar Chrome
      uses: nanasess/setup-chromedriver@v1
      with:
        chrome-version: 'latest-stable'
        chromedriver-version: 'latest-stable'

    - name: Instalar dependencias
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Ejecutar scraping
      env:
        DISPLAY: :99
      run: |
        sudo chmod 1777 /dev/shm
        Xvfb :99 -screen 0 1024x768x24 > /dev/null 2>&1 &
        python sbs_scraping.py

    - name: Subir resultados como artifact
      uses: actions/upload-artifact@v3
      with:
        name: datos-sbs-${{ github.run_number }}
        path: |
          SBS-RETASAS-*
          error_screenshot.png
        retention-days: 7

    - name: Commit y Push de resultados
      if: success() 
      run: |
        git config --global user.email "actions@github.com"  
        git config --global user.name "GitHub Actions"
        git pull --rebase
        git add SBS-RETASAS-*
        if git diff-index --quiet HEAD --; then
          echo "Sin cambios para commitear"
        else
          git commit -m "Actualización automática SBS - ${{ github.run_number }}"
          git push origin HEAD:main
        fi
      env:
        GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}